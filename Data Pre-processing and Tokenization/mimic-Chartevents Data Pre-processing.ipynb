{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input path to raw data\n",
    "mypath_input = \"/home/jupyter/datasets/chartevents/ch_events_first_24_hours_ICUSTAY/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only files with .csv suffix\n",
    "ch_chunks = list(filter(lambda k: '.csv' in k, os.listdir(mypath_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the list of files\n",
    "ch_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import episode file\n",
    "\n",
    "df =pd.read_csv(mypath_input + ch_chunks[0], \n",
    "                          dtype={'ROW_ID':'int32',\n",
    "                                'SUBJECT_ID':'int32',\n",
    "                                'HADM_ID':'int32',\n",
    "                                'ICUSTAY_ID': 'object',\n",
    "                                'ITEMID': 'int32',\n",
    "                                #'CHARTTIME':'datetime64[ns]',\n",
    "                                #'STORETIME':'datetime64[ns]',\n",
    "                                'CGID': 'object',\n",
    "                                'VALUE':'object',\n",
    "                                'VALUENUM':'object',\n",
    "                                'VALUEUOM': 'object',\n",
    "                                'WARNING':'object',\n",
    "                                'ERROR':'object',\n",
    "                                'RESULTSTATUS':'object',\n",
    "                                'STOPPED':'object',\n",
    "                                'HOSPITAL_EXPIRE_FLAG': 'int8',\n",
    "                                 'icu_tdelta':'int'}, \n",
    "                          parse_dates = ['CHARTTIME','STORETIME','ICU_INTIME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values in all columns with 'NaN'\n",
    "df = df.fillna('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import D_ITEMS file for extracting labels \n",
    "df_items = pd.read_csv(\"/home/jupyter/datasets/raw/D_ITEMS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CHARTEVENTS and D_ITEM\n",
    "df = df.merge(df_items[['ITEMID','LABEL']], left_on='ITEMID',right_on='ITEMID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each element in VALUE, VALUENUM, VALUEUOM into string\n",
    "df['VALUENUM'] = df['VALUENUM'].map(lambda x: str(x))\n",
    "df['VALUE'] = df['VALUE'].map(lambda x: str(x))\n",
    "df['VALUEUOM'] = df['VALUEUOM'].map(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate VALUENUM and VALUEUOM\n",
    "df['VALUE_NUM_UOM'] = df['VALUENUM']+'-'+df['VALUEUOM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all records with missing STORETIME\n",
    "df = df.dropna(subset=['STORETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if VALUENUM and VALUEUOM are not missing then replace VALUE with VALUE_NUM_UOM \n",
    "df.loc[(df['VALUENUM'] != 'NaN') & (df['VALUEUOM'] != 'NaN'),['VALUE']] = df['VALUE_NUM_UOM']\n",
    "\n",
    "# if VALUE is missing and VALUENUM is not missing then replace VALUE with VALUENUM  \n",
    "df.loc[(df['VALUE'] == 'NaN' ) & (df['VALUENUM'] != 'NaN'),['VALUE']] = df['VALUENUM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate ITEMID with \n",
    "df['ITEMID'] = df['ITEMID'].map(lambda x: str(x))\n",
    "df['VALUE'] = df['ITEMID']+'-'+df['VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop Columns\n",
    "df = df.drop(labels=['ROW_ID','CHARTTIME','CGID','VALUENUM','VALUEUOM','WARNING','ERROR','RESULTSTATUS','STOPPED','ICU_INTIME','VALUE_NUM_UOM','ITEMID'],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove special charecters from columns LABEL\n",
    "df['LABEL'] = df['LABEL'].str.replace('[!\"#$%&()*+,;<=>?@[\\]^_`{|}~\\n-]', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split elements in LABEL on whitespace\n",
    "df['LABEL'] = df['LABEL'].map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists\n",
    "df['VALUE'] = df['VALUE'].map(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns\n",
    "df['event'] = df['LABEL']+df['VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop labels \n",
    "df = df.drop(labels=['VALUE','LABEL'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert STORETIME to string to save date correctly in JSON\n",
    "df['STORETIME'] = df['STORETIME'].map(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export the dataframe to JSON format\n",
    "#df.to_json('../datasets/testdir/test.json',orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.read_json('../datasets/testdir/test.json',orient = 'records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to iterate and tranform all episode files in the way displayed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = pd.read_csv(\"/home/jupyter/datasets/raw/D_ITEMS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath_input = \"/home/jupyter/datasets/chartevents/ch_events_first_24_hours_ICUSTAY/\"\n",
    "mypath_output = \"/home/jupyter/datasets/chartevents/tokenized/ch_events_chunks_ungrouped/\"\n",
    "import os\n",
    "os.makedirs(mypath_output, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list = list(filter(lambda k: '.csv' in k, os.listdir(mypath_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in tqdm(chunk_list):\n",
    "    #import episode file\n",
    "\n",
    "    df =pd.read_csv(mypath_input + i, \n",
    "                              dtype={'ROW_ID':'int32',\n",
    "                                    'SUBJECT_ID':'int32',\n",
    "                                    'HADM_ID':'int32',\n",
    "                                    'ICUSTAY_ID': 'object',\n",
    "                                    'ITEMID': 'int32',\n",
    "                                    #'CHARTTIME':'datetime64[ns]',\n",
    "                                    #'STORETIME':'datetime64[ns]',\n",
    "                                    'CGID': 'object',\n",
    "                                    'VALUE':'object',\n",
    "                                    'VALUENUM':'object',\n",
    "                                    'VALUEUOM': 'object',\n",
    "                                    'WARNING':'object',\n",
    "                                    'ERROR':'object',\n",
    "                                    'RESULTSTATUS':'object',\n",
    "                                    'STOPPED':'object',\n",
    "                                    'HOSPITAL_EXPIRE_FLAG': 'int8',\n",
    "                                     'icu_tdelta':'int'}, \n",
    "                              parse_dates = ['CHARTTIME','STORETIME','ICU_INTIME'])\n",
    "\n",
    "\n",
    "    # fill missing values in all columns with 'NaN'\n",
    "    df = df.fillna('NaN')\n",
    "\n",
    "    # Merge CHARTEVENTS and D_ITEM\n",
    "    df = df.merge(df_items[['ITEMID','LABEL']], left_on='ITEMID',right_on='ITEMID')\n",
    "\n",
    "    # Convert each element in VALUE, VALUENUM, VALUEUOM into string\n",
    "    df['VALUENUM'] = df['VALUENUM'].map(lambda x: str(x))\n",
    "    df['VALUE'] = df['VALUE'].map(lambda x: str(x))\n",
    "    df['VALUEUOM'] = df['VALUEUOM'].map(lambda x: str(x))\n",
    "\n",
    "    # Concatenate VALUENUM and VALUEUOM\n",
    "    df['VALUE_NUM_UOM'] = df['VALUENUM']+'-'+df['VALUEUOM']\n",
    "\n",
    "    # Drop all records with missing STORETIME\n",
    "    df = df.dropna(subset=['STORETIME'])\n",
    "\n",
    "    # if VALUENUM and VALUEUOM are not missing then replace VALUE with VALUE_NUM_UOM \n",
    "    df.loc[(df['VALUENUM'] != 'NaN') & (df['VALUEUOM'] != 'NaN'),['VALUE']] = df['VALUE_NUM_UOM']\n",
    "\n",
    "    # if VALUE is missing and VALUENUM is not missing then replace VALUE with VALUENUM  \n",
    "    df.loc[(df['VALUE'] == 'NaN' ) & (df['VALUENUM'] != 'NaN'),['VALUE']] = df['VALUENUM']\n",
    "    \n",
    "    # Concatenate ITEMID with \n",
    "    df['ITEMID'] = df['ITEMID'].map(lambda x: str(x))\n",
    "    df['VALUE'] = df['ITEMID']+'-'+df['VALUE']\n",
    "\n",
    "    # Drop Columns\n",
    "    df = df.drop(labels=['ROW_ID','CHARTTIME','CGID','VALUENUM','VALUEUOM','WARNING','ERROR','RESULTSTATUS','STOPPED','ICU_INTIME','VALUE_NUM_UOM','ITEMID'],axis='columns')\n",
    "\n",
    "    # drop duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Remove special charecters from columns LABEL\n",
    "    df['LABEL'] = df['LABEL'].str.replace('[!\"#$%&()*+,;<=>?@[\\]^_`{|}~\\n-]', ' ')\n",
    "\n",
    "    # Split elements in LABEL on whitespace\n",
    "    df['LABEL'] = df['LABEL'].map(lambda x: x.split())\n",
    "\n",
    "    # Convert to lists\n",
    "    df['VALUE'] = df['VALUE'].map(lambda x: [x])\n",
    "\n",
    "    # Add columns\n",
    "    df['event'] = df['LABEL']+df['VALUE']\n",
    "\n",
    "    # Drop labels \n",
    "    df = df.drop(labels=['VALUE','LABEL'], axis='columns')\n",
    "\n",
    "    # convert STORETIME to string to save date correctly in JSON\n",
    "    df['STORETIME'] = df['STORETIME'].map(lambda x: str(x))\n",
    "\n",
    "    # export the dataframe to JSON format\n",
    "    df.to_json(mypath_output+i.replace('.csv','.json'),orient = 'records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mypath_output = \"/home/jupyter/datasets/chartevents/tokenized/ch_events_chunks_ungrouped/\"\n",
    "pd.read_json(mypath_output+'ch_events_24H_icu_000000000000.json').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
