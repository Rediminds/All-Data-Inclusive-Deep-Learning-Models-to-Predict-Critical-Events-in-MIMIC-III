{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath_input = '/home/jupyter/datasets/training_data/data_before_24hrs_icu/data_grouped_HADM_ID/all_events.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath_output = \"/home/jupyter/datasets/training_data/data_before_24hrs_icu/data_grouped_HADM_ID/padded_arrays/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(mypath_input).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'events':'event'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['HOSPITAL_EXPIRE_FLAG']==1].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ICUTAYS.csv\n",
    "df_icu =  pd.read_csv('/home/jupyter/datasets/raw/ICUSTAYS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping ICUstays with LOS<1\n",
    "df_icu = df_icu[df_icu['LOS'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.HADM_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['HADM_ID'].isin(df_icu['HADM_ID'].unique())]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_icu[['HADM_ID','LOS']], how='left',left_on='HADM_ID', right_on='HADM_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove records with LOS missing\n",
    "df = df[df['LOS'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary LOS i.e. >7 or 7\n",
    "df['LOS'] = df['LOS'].map(lambda x: 0 if x < 7 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of patients {}\".format(len(df.SUBJECT_ID.unique())))\n",
    "print(\"number of admissions {}\".format(len(df.HADM_ID.unique())))\n",
    "print(\"number of in-hospital mortality {}\".format(sum(df['HOSPITAL_EXPIRE_FLAG'])))\n",
    "print('patient with atleast 7 days icu stay {}'.format(sum(df['LOS'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a freq list of SUBJECT_ID\n",
    "df_subject_frq = pd.DataFrame(df['SUBJECT_ID'].value_counts().reset_index().rename(columns = {'SUBJECT_ID':'freq', 'index':'SUBJECT_ID'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split SubjectID into train/valid/test\n",
    "X_train, X_test = train_test_split(df_subject_frq.SUBJECT_ID,test_size=0.15, random_state=1234)\n",
    "\n",
    "X_train, X_valid = train_test_split(X_train,test_size=0.15, random_state=1234)\n",
    "\n",
    "print(len(X_train),len(X_valid),len(X_test))\n",
    "print(len(df[df.SUBJECT_ID.isin(X_train)]),len(df[df.SUBJECT_ID.isin(X_valid)]),len(df[df.SUBJECT_ID.isin(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export the list of hospital admission id for train , valid, test\n",
    "# Save SUBJECT_ID for training data\n",
    "import json\n",
    "with open(mypath_output+'trainlist_SUBJECT_ID.json', 'w') as filehandle:\n",
    "    json.dump(X_train.tolist(), filehandle)\n",
    "    \n",
    "# Save HADM_ID for training data\n",
    "import json\n",
    "with open(mypath_output+'validlist_SUBJECT_ID.json', 'w') as filehandle:\n",
    "    json.dump(X_valid.tolist(), filehandle)\n",
    "\n",
    "# Save HADM_ID for training data\n",
    "import json\n",
    "with open(mypath_output+'testlist_SUBJECT_ID.json', 'w') as filehandle:\n",
    "    json.dump(X_test.tolist(), filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ratio of in-hospital mortality in train, valid and test are {:.3} {:.3} {:.3}\".format(\n",
    "    sum(df[df.SUBJECT_ID.isin(X_train)]['HOSPITAL_EXPIRE_FLAG'])/len(df[df.SUBJECT_ID.isin(X_train)]),\n",
    "    sum(df[df.SUBJECT_ID.isin(X_valid)]['HOSPITAL_EXPIRE_FLAG'])/len(df[df.SUBJECT_ID.isin(X_valid)]),\n",
    "    sum(df[df.SUBJECT_ID.isin(X_test)]['HOSPITAL_EXPIRE_FLAG'])/len(df[df.SUBJECT_ID.isin(X_test)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ratio of icustays atleast 7 days in train, valid and test are {:.4} {:.4} {:.4}\".format(\n",
    "    sum(df[df.SUBJECT_ID.isin(X_train)]['LOS'])/len(df[df.SUBJECT_ID.isin(X_train)]),\n",
    "    sum(df[df.SUBJECT_ID.isin(X_valid)]['LOS'])/len(df[df.SUBJECT_ID.isin(X_valid)]),\n",
    "    sum(df[df.SUBJECT_ID.isin(X_test)]['LOS'])/len(df[df.SUBJECT_ID.isin(X_test)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes input as a pd.series of lists and determines longest length of a list in the series\n",
    "def cal_max_len(x):\n",
    "    a = []\n",
    "    for i in range(len(x)):\n",
    "        a.append(len(x[i]))\n",
    "    max_len = int(max(a))\n",
    "    pctl_999 = int(np.percentile(a, 99.9))\n",
    "    return max_len, pctl_999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary to hold max length of list in each column\n",
    "max_len_dict = {}\n",
    "pctl_999_dict = {}\n",
    "for i in df.columns:\n",
    "    if 'event' in i:\n",
    "        max_len_dict.update({i:cal_max_len(df[i])[0]})\n",
    "        pctl_999_dict.update({i:cal_max_len(df[i])[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl_999_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_len_dict)\n",
    "# Save dict containing max length of each event type for training data\n",
    "import json\n",
    "with open(mypath_output+'max_padlen_dict.json', 'w') as filehandle:\n",
    "    json.dump(max_len_dict, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl_999_dict\n",
    "print(pctl_999_dict)\n",
    "# Save dict containing 99.9 pctl length of each event type for training data\n",
    "import json\n",
    "with open(mypath_output+'pctl999_padlen_dict.json', 'w') as filehandle:\n",
    "    json.dump(pctl_999_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit Tokenizer for Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training, valid, test Dataset\n",
    "df_train = df[df.SUBJECT_ID.isin(X_train)].reset_index(drop = True)\n",
    "df_valid = df[df.SUBJECT_ID.isin(X_valid)].reset_index(drop = True)\n",
    "df_test = df[df.SUBJECT_ID.isin(X_test)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output path\n",
    "tokenizer_path = \"/home/jupyter/output/tokenizer/\"\n",
    "import os\n",
    "os.makedirs(tokenizer_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training tokenizers on train dataset\n",
    "tokenizer_dict = {} \n",
    "for i in df.columns:\n",
    "    if 'event' in i:\n",
    "        # initiate tokenizer\n",
    "        t = Tokenizer(lower=True,split=',', filters= '')\n",
    "        # Fit tokenizer on event\n",
    "        t.fit_on_texts(df_train[i])\n",
    "        tokenizer_dict.update({i:t})\n",
    "        print(\"Tokenizer built for {}\".format(i))\n",
    "print('All tokenizers are built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the Chartevents tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_chartevents.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['event'],handle)  \n",
    "    \n",
    "#saving the Inputevents_cv tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_inputevents_cv.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['inputevents_cv'],handle) \n",
    "    \n",
    "#saving the Inputevents_mv tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_inputevents_mv.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['inputevents_mv'],handle) \n",
    "\n",
    "#saving the Labevents tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_labevents.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['labevents'],handle) \n",
    "    \n",
    "#saving the Microbioevents tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_microbioevents.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['microbioevents'],handle) \n",
    "    \n",
    "#saving the Notevents tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_noteevents.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['noteevents'],handle) \n",
    "\n",
    "#saving the Outputevents tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_outputevents.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['outputevents'],handle) \n",
    "\n",
    "#saving the Prescriptionevents tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_prescriptionevents.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['prescriptionevents'],handle) \n",
    "    \n",
    "#saving the Procedureevents tokenizer to disk\n",
    "with open(tokenizer_path+\"tokenizer_procedureevents.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer_dict['procedureevents'],handle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Encode Train Data, Valid Data, Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove low count words from tokenizer\n",
    "def tokenizer_low_count(tokenizer):\n",
    "    t = tokenizer\n",
    "    #initialize unknown token\n",
    "    t.oov_token = 'UNK'\n",
    "    # add \"UNK\" token and an integer value for it to the tokenizer word index\n",
    "    t.word_index.update({'UNK':len(t.word_index)+1})\n",
    "    # Create a list of tokens that occur only once\n",
    "    low_count_words = []\n",
    "    for k,v in t.word_counts.items():\n",
    "        if v==1:\n",
    "            low_count_words.append(k)\n",
    "    #Removed words with low count from tokenizer\n",
    "    for w in low_count_words:\n",
    "        del t.word_index[w]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_procedureevents.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle)\n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['procedureevents'] = df_train['procedureevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['procedureevents'] = df_valid['procedureevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['procedureevents'] = df_test['procedureevents'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_chartevents.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['event'] = df_train['event'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['event'] = df_valid['event'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['event'] = df_test['event'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_inputevents_cv.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['inputevents_cv'] = df_train['inputevents_cv'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['inputevents_cv'] = df_valid['inputevents_cv'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['inputevents_cv'] = df_test['inputevents_cv'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_inputevents_mv.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['inputevents_mv'] = df_train['inputevents_mv'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['inputevents_mv'] = df_valid['inputevents_mv'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['inputevents_mv'] = df_test['inputevents_mv'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_labevents.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['labevents'] = df_train['labevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['labevents'] = df_valid['labevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['labevents'] = df_test['labevents'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_microbioevents.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['microbioevents'] = df_train['microbioevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['microbioevents'] = df_valid['microbioevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['microbioevents'] = df_test['microbioevents'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_noteevents.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['noteevents'] = df_train['noteevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['noteevents'] = df_valid['noteevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['noteevents'] = df_test['noteevents'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_outputevents.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['outputevents'] = df_train['outputevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['outputevents'] = df_valid['outputevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['outputevents'] = df_test['outputevents'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer form disk\n",
    "with open(tokenizer_path+\"tokenizer_prescriptionevents.pickle\", 'rb') as handle:\n",
    "    t = pickle.load(handle) \n",
    "\n",
    "# Removing tokens with low count\n",
    "tokenizer_low_count(t)\n",
    "\n",
    "# replacing tokens with their integer codes\n",
    "df_train['prescriptionevents'] = df_train['prescriptionevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_valid['prescriptionevents'] = df_valid['prescriptionevents'].map(lambda y: t.texts_to_sequences([y])[0])\n",
    "df_test['prescriptionevents'] = df_test['prescriptionevents'].map(lambda y: t.texts_to_sequences([y])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the integer encoded data to disk\n",
    "# create output path\n",
    "file_save_path = '/home/jupyter/datasets/training_data/data_before_24hrs_icu/data_grouped_HADM_ID/train_test_valid/'\n",
    "import os\n",
    "os.makedirs(file_save_path, exist_ok=True)\n",
    "# export the dataframe to JSON format\n",
    "df_train.to_json(file_save_path+\"all_events_train.json\",orient = 'records')\n",
    "df_valid.to_json(file_save_path+\"all_events_valid.json\",orient = 'records') \n",
    "df_test.to_json(file_save_path+\"all_events_test.json\",orient = 'records') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
