{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "import tensorflow.keras as keras\n",
    "from sklearn import metrics\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend\n",
    "from time import time\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test dataset\n",
    "test_input = \"/home/jupyter/datasets/training_data/data_before_24hrs_icu/data_grouped_HADM_ID/padded_arrays/all_events_test.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test dataset for number of records\n",
    "test_records = sum(1 for _ in tf.python_io.tf_record_iterator(test_input))\n",
    "print(test_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign file path to tfrecords dataset\n",
    "test_filenames = [test_input]\n",
    "test_dataset = tf.data.TFRecordDataset(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Parse Functions\n",
    "\n",
    "# Create a description of the features.  \n",
    "feature_description = {\n",
    "    'HOSPITAL_EXPIRE_FLAG': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'LOS': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    'feature1': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature2': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature3': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature4': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature5': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature6': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature7': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature8': tf.VarLenFeature(dtype=tf.int64),\n",
    "    'feature9': tf.VarLenFeature(dtype=tf.int64)\n",
    "}\n",
    "\n",
    "# Parse function for All Events Mortality\n",
    "def _parse_function_all_events_mortality(example_proto):\n",
    "    x = tf.parse_single_example(example_proto, feature_description)\n",
    "    label = tf.cast(x['HOSPITAL_EXPIRE_FLAG'],dtype='int32')\n",
    "    ch_events = tf.cast(tf.sparse.to_dense(x['feature1']),dtype='int32')\n",
    "    inputcv_events = tf.cast(tf.sparse.to_dense(x['feature2']),dtype='int32')\n",
    "    inputmv_events = tf.cast(tf.sparse.to_dense(x['feature3']),dtype='int32')\n",
    "    lab_events = tf.cast(tf.sparse.to_dense(x['feature4']),dtype='int32')\n",
    "    microbio_events = tf.cast(tf.sparse.to_dense(x['feature5']),dtype='int32')\n",
    "    note_events = tf.cast(tf.sparse.to_dense(x['feature6']),dtype='int32')\n",
    "    output_events = tf.cast(tf.sparse.to_dense(x['feature7']),dtype='int32')\n",
    "    prescription_events = tf.cast(tf.sparse.to_dense(x['feature8']),dtype='int32')\n",
    "    procedure_events = tf.cast(tf.sparse.to_dense(x['feature9']),dtype='int32')\n",
    "    \n",
    "    return ((ch_events,\n",
    "                 inputcv_events,\n",
    "                 inputmv_events,\n",
    "                 lab_events,\n",
    "                 microbio_events,\n",
    "                 note_events,\n",
    "                 output_events,\n",
    "                 prescription_events,\n",
    "                 procedure_events),\n",
    "            label)#{'labels':label, 'ch_events':ch_events}\n",
    "\n",
    "# Parse function for Chart Events Mortality\n",
    "def _parse_function_ch_events_mortality(example_proto):\n",
    "    x = tf.parse_single_example(example_proto, feature_description)\n",
    "    label = tf.cast(x['HOSPITAL_EXPIRE_FLAG'],dtype='int32')\n",
    "    ch_events = tf.cast(tf.sparse.to_dense(x['feature1']),dtype='int32')\n",
    "    inputcv_events = tf.cast(tf.sparse.to_dense(x['feature2']),dtype='int32')\n",
    "    inputmv_events = tf.cast(tf.sparse.to_dense(x['feature3']),dtype='int32')\n",
    "    lab_events = tf.cast(tf.sparse.to_dense(x['feature4']),dtype='int32')\n",
    "    microbio_events = tf.cast(tf.sparse.to_dense(x['feature5']),dtype='int32')\n",
    "    note_events = tf.cast(tf.sparse.to_dense(x['feature6']),dtype='int32')\n",
    "    output_events = tf.cast(tf.sparse.to_dense(x['feature7']),dtype='int32')\n",
    "    prescription_events = tf.cast(tf.sparse.to_dense(x['feature8']),dtype='int32')\n",
    "    procedure_events = tf.cast(tf.sparse.to_dense(x['feature9']),dtype='int32')\n",
    "    \n",
    "    return ((ch_events),label)#{'labels':label, 'ch_events':ch_events}\n",
    "\n",
    "# Parse function for All Events Length of stay\n",
    "def _parse_function_all_events_LOS(example_proto):\n",
    "    x = tf.parse_single_example(example_proto, feature_description)\n",
    "    label = tf.cast(x['LOS'],dtype='int32')\n",
    "    ch_events = tf.cast(tf.sparse.to_dense(x['feature1']),dtype='int32')\n",
    "    inputcv_events = tf.cast(tf.sparse.to_dense(x['feature2']),dtype='int32')\n",
    "    inputmv_events = tf.cast(tf.sparse.to_dense(x['feature3']),dtype='int32')\n",
    "    lab_events = tf.cast(tf.sparse.to_dense(x['feature4']),dtype='int32')\n",
    "    microbio_events = tf.cast(tf.sparse.to_dense(x['feature5']),dtype='int32')\n",
    "    note_events = tf.cast(tf.sparse.to_dense(x['feature6']),dtype='int32')\n",
    "    output_events = tf.cast(tf.sparse.to_dense(x['feature7']),dtype='int32')\n",
    "    prescription_events = tf.cast(tf.sparse.to_dense(x['feature8']),dtype='int32')\n",
    "    procedure_events = tf.cast(tf.sparse.to_dense(x['feature9']),dtype='int32')\n",
    "    \n",
    "    return ((ch_events,\n",
    "                 inputcv_events,\n",
    "                 inputmv_events,\n",
    "                 lab_events,\n",
    "                 microbio_events,\n",
    "                 note_events,\n",
    "                 output_events,\n",
    "                 prescription_events,\n",
    "                 procedure_events),\n",
    "            label)#{'labels':label, 'ch_events':ch_events}\n",
    "\n",
    "# Parse Funtion for Chartevents Length of Stay\n",
    "def _parse_function_ch_events_LOS(example_proto):\n",
    "    x = tf.parse_single_example(example_proto, feature_description)\n",
    "    label = tf.cast(x['LOS'],dtype='int32')\n",
    "    ch_events = tf.cast(tf.sparse.to_dense(x['feature1']),dtype='int32')\n",
    "    inputcv_events = tf.cast(tf.sparse.to_dense(x['feature2']),dtype='int32')\n",
    "    inputmv_events = tf.cast(tf.sparse.to_dense(x['feature3']),dtype='int32')\n",
    "    lab_events = tf.cast(tf.sparse.to_dense(x['feature4']),dtype='int32')\n",
    "    microbio_events = tf.cast(tf.sparse.to_dense(x['feature5']),dtype='int32')\n",
    "    note_events = tf.cast(tf.sparse.to_dense(x['feature6']),dtype='int32')\n",
    "    output_events = tf.cast(tf.sparse.to_dense(x['feature7']),dtype='int32')\n",
    "    prescription_events = tf.cast(tf.sparse.to_dense(x['feature8']),dtype='int32')\n",
    "    procedure_events = tf.cast(tf.sparse.to_dense(x['feature9']),dtype='int32')\n",
    "    \n",
    "    return ((ch_events),label)#{'labels':label, 'ch_events':ch_events}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom layer funtion  \n",
    "def call(inputs, mask=None):\n",
    "    steps_axis = 1\n",
    "    if mask is not None:\n",
    "        mask = math_ops.cast(mask, backend.floatx())\n",
    "        input_shape = inputs.shape.as_list()\n",
    "        broadcast_shape = [-1, input_shape[steps_axis], 1]\n",
    "        mask = array_ops.reshape(mask, broadcast_shape)\n",
    "        inputs *= mask\n",
    "        return backend.sum(inputs, axis=steps_axis) / (math_ops.reduce_sum(mask, axis=steps_axis)+backend.epsilon())\n",
    "    else:\n",
    "        return backend.mean(inputs, axis=steps_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Models which need to be evaluvated\n",
    "# Hyper tuned models as loaded into the script. \n",
    "#If you used AI Platform to hypertune your models then download the models and modify the paths below.\n",
    "\n",
    "# Load Mortality model traine dusing AllSources/All Events\n",
    "model_path = \"/home/jupyter/output/model/hypertuned_validation/Mortality_all_events/model/1578250546/\"\n",
    "all_events_mortality = tf.contrib.saved_model.load_keras_model(model_path)\n",
    "all_events_mortality.compile(optimizer=keras.optimizers.Adam(), loss= 'binary_crossentropy')\n",
    "\n",
    "# Load Mortality model traine dusing ChartEvents\n",
    "model_path = \"/home/jupyter/output/model/hypertuned_validation2/Mortality_ch_events/model/1578380697/\"\n",
    "ch_events_mortality = tf.contrib.saved_model.load_keras_model(model_path)\n",
    "ch_events_mortality.compile(optimizer=keras.optimizers.Adam(), loss= 'binary_crossentropy')\n",
    "\n",
    "# Load LOS model traine dusing AllSources/All Events\n",
    "model_path = \"/home/jupyter/output/model/hypertuned_validation/LOS_all_events/model/1578262133/\"\n",
    "all_events_LOS = tf.contrib.saved_model.load_keras_model(model_path)\n",
    "all_events_LOS.compile(optimizer=keras.optimizers.Adam(), loss= 'binary_crossentropy')\n",
    "\n",
    "# Load LOS model traine dusing ChartEvents\n",
    "model_path = \"/home/jupyter/output/model/hypertuned_validation/LOS_ch_events/model/1578224943/\"\n",
    "ch_events_LOS = tf.contrib.saved_model.load_keras_model(model_path)\n",
    "ch_events_LOS.compile(optimizer=keras.optimizers.Adam(), loss= 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define funtion to run inference on test set\n",
    "def run_inference(model, _parse_function):\n",
    "    batch_size = 128\n",
    "    test_parsed_dataset = test_dataset.map(_parse_function,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    test_parsed_dataset = test_parsed_dataset.batch(batch_size)\n",
    "    test_parsed_dataset = test_parsed_dataset.repeat()\n",
    "\n",
    "    results = model.predict(test_parsed_dataset,\n",
    "                            steps=int(test_records/batch_size)+1,\n",
    "                            verbose=1)\n",
    "\n",
    "    test = test_dataset.map(_parse_function,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    n = test.make_one_shot_iterator().get_next()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    output_labels=[]\n",
    "\n",
    "    for i in range(test_records):\n",
    "      value=sess.run(n)\n",
    "      output_labels.append(value[1])\n",
    "\n",
    "    len(output_labels)\n",
    "\n",
    "    result = pd.DataFrame(results, columns=['Predictions'])\n",
    "\n",
    "    result['Predictions'] = round(result['Predictions'],3)\n",
    "\n",
    "    result['pred_1'] = round(result['Predictions'])\n",
    "\n",
    "    result['actual'] = output_labels\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all_mortality = run_inference(model=all_events_mortality, _parse_function=_parse_function_all_events_mortality)\n",
    "result_ch_mortality = run_inference(model=ch_events_mortality, _parse_function=_parse_function_ch_events_mortality)\n",
    "result_all_LOS = run_inference(model=all_events_LOS, _parse_function=_parse_function_all_events_LOS)\n",
    "result_ch_LOS = run_inference(model=ch_events_LOS, _parse_function=_parse_function_ch_events_LOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to create AUC-ROC, PR-CURVE, Calibration Curve\n",
    "def print_roc_pr_calibration_curve(model, _parse_function):\n",
    "    result = run_inference(model = model, _parse_function=_parse_function)\n",
    "    \n",
    "    # Plotting AUROC Curve - Calculatae Metrics\n",
    "    from sklearn import metrics\n",
    "    y_ACTUAL= result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    # Plotting AUROC Curve - Plot Curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    roc_curve = plt.figure(figsize=(8,6))\n",
    "    plt.title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "    plt.xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)\n",
    "    \n",
    "    # Plotting Precision-Recall Curve - Calculate Metrics\n",
    "    y_ACTUAL = result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    yhat = result['pred_1']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    mortality_ratio = round(sum(result['actual'])/(len(result['actual'])),2)\n",
    "     # Plotting Precision-Recall Curve - PLot Curve\n",
    "    pr_curve = plt.figure(figsize=(8,6))\n",
    "    plt.title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Precision',fontdict={\"size\":15})\n",
    "    plt.xlabel('Recall',fontdict={\"size\":15})\n",
    "    plt.plot(recall, precision, label = 'F1=%.2f  AUC=%.2f' % (f1, prc_auc))\n",
    "    positive_class_ratio = mortality_ratio\n",
    "    plt.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "\n",
    "    # Plotting Calibration curve\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    x, y = calibration_curve(result['actual'], result['Predictions'], n_bins = 10, strategy='uniform')\n",
    "    import matplotlib.pyplot as plt\n",
    "    claibration_curve = plt.figure(figsize=(8,6))\n",
    "    plt.title('Calibration Curve',fontdict={\"size\":20})\n",
    "    plt.plot(x,y, marker = 'o', color = 'orange')\n",
    "    plt.plot([0, 1], [0, 1],linestyle='--',color='black', label = 'Perfectly Calibrated')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "    plt.ylabel('Fraction of Positives',fontdict={\"size\":15})\n",
    "    plt.xlabel('Mean Predicted value',fontdict={\"size\":15})\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot AUCROC, PR-AUC, Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting AUROC Curve - Calculatae Metrics\n",
    "from sklearn import metrics\n",
    "def calc_aucroc_data(result):\n",
    "    y_ACTUAL= result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    yhat = result['pred_1']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    mortality_ratio = round(sum(result['actual'])/(len(result['actual'])),2)\n",
    "    return recall, precision, f1, prc_auc, mortality_ratio\n",
    "\n",
    "fpr_am, tpr_am, thresholds_am, roc_auc_am = calc_aucroc_data(result_all_mortality)\n",
    "fpr_cm, tpr_cm, thresholds_cm, roc_auc_cm = calc_aucroc_data(result_ch_mortality)\n",
    "recall_am, precision_am, f1_am, prc_auc_am, mortality_ratio = calc_aucpr_data(result_all_mortality)\n",
    "recall_cm, precision_cm, f1_cm, prc_auc_cm, mortality_ratio = calc_aucpr_data(result_ch_mortality)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "fig.suptitle('Comparision of AUC-ROC and PR-AUC Curves', fontsize=20, y = 1.03)\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "ax1.plot(fpr_am, tpr_am, label = 'AUC-ROC IHM-AS = %0.2f' % roc_auc_am)\n",
    "ax1.plot(fpr_cm, tpr_cm, label = 'AUC-ROC IHM-CS = %0.2f' % roc_auc_cm)\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "ax2.plot(recall_am, precision_am, label = 'PR-AUC IHM-AS=%.2f' % (prc_auc_am))\n",
    "ax2.plot(recall_cm, precision_cm, label = 'PR-AUC IHM-CS=%.2f' % (prc_auc_cm))\n",
    "positive_class_ratio = mortality_ratio\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Plotting Calibration curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "x_am, y_am = calibration_curve(result_all_mortality['actual'], result_all_mortality['Predictions'], n_bins = 10, strategy='uniform')\n",
    "x_cm, y_cm = calibration_curve(result_ch_mortality['actual'], result_ch_mortality['Predictions'], n_bins = 10, strategy='uniform')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "fig.suptitle('Comparision of Calibration Plots (Reliability Curves)', fontsize=20)\n",
    "ax1.plot(x_am, y_am, marker = 'o', color = 'tab:blue', label = 'IHM-AS')\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='black', label = 'Perfectly Calibrated')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "ax1.set_ylabel('Fraction of Positives',fontdict={\"size\":15})\n",
    "ax1.set_xlabel('Mean Predicted value',fontdict={\"size\":15})\n",
    "\n",
    "\n",
    "ax2.plot(x_cm, y_cm, marker = 'o', color = 'tab:orange', label = 'IHM-CS')\n",
    "ax2.plot([0, 1], [0, 1],linestyle='--',color='black', label = 'Perfectly Calibrated')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "ax2.set_ylabel('Fraction of Positives',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Mean Predicted value',fontdict={\"size\":15})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_am)\n",
    "print(roc_auc_cm)\n",
    "print(prc_auc_am)\n",
    "print(prc_auc_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting AUROC Curve - Calculatae Metrics\n",
    "from sklearn import metrics\n",
    "def calc_aucroc_data(result):\n",
    "    y_ACTUAL= result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    yhat = result['pred_1']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    mortality_ratio = round(sum(result['actual'])/(len(result['actual'])),2)\n",
    "    return recall, precision, f1, prc_auc, mortality_ratio\n",
    "\n",
    "fpr_am, tpr_am, thresholds_am, roc_auc_am = calc_aucroc_data(result_all_LOS)\n",
    "fpr_cm, tpr_cm, thresholds_cm, roc_auc_cm = calc_aucroc_data(result_ch_LOS)\n",
    "recall_am, precision_am, f1_am, prc_auc_am, mortality_ratio = calc_aucpr_data(result_all_LOS)\n",
    "recall_cm, precision_cm, f1_cm, prc_auc_cm, mortality_ratio = calc_aucpr_data(result_ch_LOS)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "fig.suptitle('Comparision of AUC-ROC and PR-AUC Curves', fontsize=20, y = 1.03)\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "ax1.plot(fpr_am, tpr_am, label = 'AUC-ROC LOS-AS = %0.2f' % roc_auc_am)\n",
    "ax1.plot(fpr_cm, tpr_cm, label = 'AUC-ROC LOS-CS = %0.2f' % roc_auc_cm)\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "ax2.plot(recall_am, precision_am, label = 'PR-AUC LOS-AS=%.2f' % (prc_auc_am))\n",
    "ax2.plot(recall_cm, precision_cm, label = 'PR-AUC LOS-CS=%.2f' % (prc_auc_cm))\n",
    "positive_class_ratio = mortality_ratio\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Plotting Calibration curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "x_am, y_am = calibration_curve(result_all_LOS['actual'], result_all_LOS['Predictions'], n_bins = 10, strategy='uniform')\n",
    "x_cm, y_cm = calibration_curve(result_ch_LOS['actual'], result_ch_LOS['Predictions'], n_bins = 10, strategy='uniform')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "fig.suptitle('Comparision of Calibration Plots (Reliability Curves)', fontsize=20)\n",
    "ax1.plot(x_am, y_am, marker = 'o', color = 'tab:blue', label = 'LOS-AS')\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='black', label = 'Perfectly Calibrated')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "ax1.set_ylabel('Fraction of Positives',fontdict={\"size\":15})\n",
    "ax1.set_xlabel('Mean Predicted value',fontdict={\"size\":15})\n",
    "\n",
    "\n",
    "ax2.plot(x_cm, y_cm, marker = 'o', color = 'tab:orange', label = 'LOS-CS')\n",
    "ax2.plot([0, 1], [0, 1],linestyle='--',color='black', label = 'Perfectly Calibrated')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "ax2.set_ylabel('Fraction of Positives',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Mean Predicted value',fontdict={\"size\":15})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_am)\n",
    "print(roc_auc_cm)\n",
    "print(prc_auc_am)\n",
    "print(prc_auc_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_auc(result):    \n",
    "    from sklearn import metrics\n",
    "    y_ACTUAL= result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    y_ACTUAL = result['actual']\n",
    "    scores_prob = result['Predictions']\n",
    "    yhat = result['pred_1']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)   \n",
    "    return roc_auc, prc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapped_AUC(result):\n",
    "    from sklearn.utils import resample\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    n_iter = 10000\n",
    "    roc_auc = list()\n",
    "    prc_auc = list()\n",
    "\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        #Calculating AUROC for each sample\n",
    "        result_sample = resample(result, n_samples = len(result), random_state = i)\n",
    "        y_ACTUAL= result_sample['actual']\n",
    "        scores_prob = result_sample['Predictions']\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        roc_auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "        #calculate AUPRC for each sample\n",
    "        y_ACTUAL = result_sample['actual']\n",
    "        scores_prob = result_sample['Predictions']\n",
    "        yhat = result_sample['pred_1']\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        prc_auc.append(metrics.auc(recall,precision))\n",
    "    \n",
    "    return roc_auc, prc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "roc_auc_all_mortality,pr_auc_all_mortality  = bootstrapped_AUC(result_all_mortality)\n",
    "roc_auc_ch_mortality,pr_auc_ch_mortality  = bootstrapped_AUC(result_ch_mortality)\n",
    "roc_auc_all_LOS,pr_auc_all_LOS  = bootstrapped_AUC(result_all_LOS)\n",
    "roc_auc_ch_LOS,pr_auc_ch_LOS  = bootstrapped_AUC(result_ch_LOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AUC Diff statistical Significance of Mortality Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'roc_auc_all_mortality': roc_auc_all_mortality,\n",
    "        'roc_auc_ch_mortality': roc_auc_ch_mortality,\n",
    "        'pr_auc_all_mortality': pr_auc_all_mortality,\n",
    "        'pr_auc_ch_mortality' : pr_auc_ch_mortality\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mortality = pd.DataFrame(dict)\n",
    "df_mortality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mortality.describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_mortality.describe(percentiles=[0.025,0.975])\n",
    "for i in df_mortality.columns:\n",
    "    print(i+' 95%CI: {:.4f}({:.4f},{:.4f})'.format(x[i]['mean'], x[i]['2.5%'], x[i]['97.5%']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax = sns.distplot(roc_auc_ch_mortality)\n",
    "ax = sns.distplot(roc_auc_all_mortality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax = sns.distplot(pr_auc_ch_mortality)\n",
    "ax = sns.distplot(pr_auc_all_mortality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mortality\n",
    "df_mortality['roc_auc_diff_mortality'] = df_mortality['roc_auc_all_mortality']-df_mortality['roc_auc_ch_mortality']\n",
    "df_mortality['pr_auc_diff_mortality'] = df_mortality['pr_auc_all_mortality']-df_mortality['pr_auc_ch_mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mortality.describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AUC Diff statistical Significance of LOS Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'roc_auc_all_LOS': roc_auc_all_LOS,\n",
    "        'roc_auc_ch_LOS': roc_auc_ch_LOS,\n",
    "        'pr_auc_all_LOS': pr_auc_all_LOS,\n",
    "        'pr_auc_ch_LOS' : pr_auc_ch_LOS\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LOS = pd.DataFrame(dict)\n",
    "df_LOS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LOS.describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_LOS.describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_LOS.columns:\n",
    "    print(i+' 95%CI: {:.4f}({:.4f},{:.4f})'.format(x[i]['mean'], x[i]['2.5%'], x[i]['97.5%']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax = sns.distplot(roc_auc_ch_LOS)\n",
    "ax = sns.distplot(roc_auc_all_LOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ay = sns.distplot(pr_auc_ch_LOS)\n",
    "ay = sns.distplot(pr_auc_all_LOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LOS['auc_diff_LOS'] = df_LOS['roc_auc_all_LOS']-df_LOS['roc_auc_ch_LOS']\n",
    "df_LOS['pr_auc_diff_LOS'] = df_LOS['pr_auc_all_LOS']-df_LOS['pr_auc_ch_LOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LOS.describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Hypothesis Testing Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([result_all_mortality,result_ch_mortality])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perms_all_mortality = []\n",
    "perms_ch_mortality = []\n",
    "perms_all_mortality_pr = []\n",
    "perms_ch_mortality_pr = []\n",
    "dif_bootstrap_auc = []\n",
    "dif_bootstrap_pr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10000):\n",
    "    np.random.seed(i)\n",
    "    perms_all_mortality_roc, perms_all_mortality_pr = calculate_roc_auc(resample(combined, n_samples = len(result_all_mortality)))\n",
    "    perms_ch_mortality_roc, perms_ch_mortality_pr  = calculate_roc_auc(resample(combined, n_samples = len(result_ch_mortality)))\n",
    "    dif_bootstrap_auc.append(perms_all_mortality_roc-perms_ch_mortality_roc)\n",
    "    dif_bootstrap_pr.append(perms_all_mortality_pr-perms_ch_mortality_pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = sns.distplot(dif_bootstrap_auc)\n",
    "\n",
    "plt.xlabel('Difference in Likes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrapped Population (Combined data)')\n",
    "plt.show()\n",
    "\n",
    "# Observed Difference\n",
    "obs_difs = (calculate_roc_auc(result_all_mortality)[0] - calculate_roc_auc(result_ch_mortality)[0])\n",
    "print('observed difference in AUROC: {}'.format(obs_difs))\n",
    "p_value = sum(dif_bootstrap_auc >= obs_difs)/10000\n",
    "print('p-value: {}'.format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = sns.distplot(dif_bootstrap_pr)\n",
    "\n",
    "plt.xlabel('Difference in Likes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrapped Population (Combined data)')\n",
    "plt.show()\n",
    "\n",
    "# Observed Difference\n",
    "obs_difs = (calculate_roc_auc(result_all_mortality)[1] - calculate_roc_auc(result_ch_mortality)[1])\n",
    "print('observed difference in AUROC: {}'.format(obs_difs))\n",
    "p_value = sum(dif_bootstrap_pr >= obs_difs)/10000\n",
    "print('p-value: {}'.format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = sns.distplot(dif_bootstrap_auc)\n",
    "\n",
    "plt.xlabel('Difference in Likes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(obs_difs, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Hypothesis Testing LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([result_all_LOS,result_ch_LOS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perms_all_LOS = []\n",
    "perms_ch_LOS = []\n",
    "perms_all_LOS_pr = []\n",
    "perms_ch_LOS_pr = []\n",
    "dif_bootstrap_auc = []\n",
    "dif_bootstrap_pr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10000):\n",
    "    np.random.seed(i)\n",
    "    perms_all_LOS_roc, perms_all_LOS_pr = calculate_roc_auc(resample(combined, n_samples = len(result_all_LOS)))\n",
    "    perms_ch_LOS_roc, perms_ch_LOS_pr  = calculate_roc_auc(resample(combined, n_samples = len(result_ch_LOS)))\n",
    "    dif_bootstrap_auc.append(perms_all_LOS_roc-perms_ch_LOS_roc)\n",
    "    dif_bootstrap_pr.append(perms_all_LOS_pr-perms_ch_LOS_pr)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = sns.distplot(dif_bootstrap_auc)\n",
    "\n",
    "plt.xlabel('Difference in Likes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrapped Population (Combined data)')\n",
    "plt.show()\n",
    "\n",
    "# Observed Difference\n",
    "obs_difs = (calculate_roc_auc(result_all_LOS)[0] - calculate_roc_auc(result_ch_LOS)[0])\n",
    "print('observed difference in AUROC: {}'.format(obs_difs))\n",
    "p_value = sum(dif_bootstrap_auc >= obs_difs)/10000\n",
    "print('p-value: {}'.format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = sns.distplot(dif_bootstrap_pr)\n",
    "\n",
    "plt.xlabel('Difference in Likes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrapped Population (Combined data)')\n",
    "plt.show()\n",
    "\n",
    "# Observed Difference\n",
    "obs_difs = (calculate_roc_auc(result_all_LOS)[1] - calculate_roc_auc(result_ch_LOS)[1])\n",
    "print('observed difference in PRAUC: {}'.format(obs_difs))\n",
    "p_value = sum(dif_bootstrap_pr >= obs_difs)/10000\n",
    "print('p-value: {}'.format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax = sns.distplot(dif_bootstrap_auc)\n",
    "\n",
    "plt.xlabel('Difference in Likes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(obs_difs, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
